# EXP-1-PROMPT-ENGINEERING-

## Aim: 
Comprehensive Report on the Fundamentals of Generative AI and Large Language Models (LLMs)
Experiment: Develop a comprehensive report for the following exercises:

Explain the foundational concepts of Generative AI.
Focusing on Generative AI architectures. (like transformers).
Generative AI applications.
Generative AI impact of scaling in LLMs.

## Algorithm:
Define Generative AI

Identify Generative AI as a class of AI systems designed to produce new data rather than only analyze or classify existing data.

Formalize the goal: generate outputs 
ğ‘¦
y from a learned probability distribution 
ğ‘ƒ
(
ğ‘¦
âˆ£
ğ‘¥
)
P(yâˆ£x).

Differentiate from Discriminative AI

Discriminative models: learn decision boundaries 
ğ‘ƒ
(
ğ‘¦
âˆ£
ğ‘¥
)
P(yâˆ£x).

Generative models: learn the joint probability distribution 
ğ‘ƒ
(
ğ‘¥
,
ğ‘¦
)
P(x,y) and can synthesize new examples.

Example:

Discriminative â†’ â€œIs this text spam?â€

Generative â†’ â€œWrite a spam email with specific keywords.â€

Tokenization and Data Representation

Input text is broken into tokens (words, subwords, or characters).

Tokens are converted into embeddings (numerical vectors).

Example: â€œHello worldâ€ â†’ [5023, 764] (IDs) â†’ dense vectors via embedding matrix.

Training via Self-Supervision

Use massive unlabelled datasets.

Mask or predict missing tokens: model learns language structure.

Objective: minimize cross-entropy loss between predicted and actual tokens.

Formula:
\n L = âˆ’ Î£ log P(token_i | context_{i-1})

Generative Process (Next-Token Prediction)

At inference:

Provide prompt (initial sequence).

Model predicts probability distribution over possible next tokens.

Select token using a decoding strategy (greedy, top-k, top-p, beam).

Append token to sequence â†’ repeat until stop condition.

Example:

Prompt: â€œThe sun rises in theâ€

Predicted distribution: {east: 0.85, west: 0.05, north: 0.04, â€¦}

Model selects â€œeast.â€

## Output
The execution of the designed algorithms in Generative AI and Large Language Models (LLMs) produced a wide variety of outputs, which demonstrate the potential of such systems across different domains. When a simple explanatory question was asked, such as â€œWhy is the sky blue?â€, the model generated a coherent and scientifically accurate explanation: â€œThe sky looks blue because sunlight is made up of different colors. As sunlight enters Earthâ€™s atmosphere, tiny particles scatter the light. Blue light travels in shorter waves, so it is scattered more than other colors, and this makes the sky appear blue to our eyes.â€ This output highlights the modelâ€™s ability to transform complex physical phenomena into a simplified explanation suitable for a general audience.

The outputs were not limited to explanations alone but extended into summarization tasks as well. For instance, when prompted to summarize the role of Artificial Intelligence in healthcare, the model condensed long paragraphs into concise points, stating that â€œAI in healthcare enables early diagnosis, supports personalized treatments, reduces human error, and enhances efficiency in patient care.â€ This indicates the strength of LLMs in information compression, where the original text is reduced while retaining the essential meaning.

Code generation was another key outcome observed. When asked to write a Python function to check if a number is prime, the system generated a clean, executable function that iterates only up to the square root of the number, demonstrating both efficiency and correctness. The generated code was free from syntax errors and executable without modification, showing how Generative AI can support programmers in automating repetitive or logical tasks. Similarly, in creative writing, the system displayed versatility by producing poetic lines such as: â€œRise each day with hope anew, the strength you seek lies within you.â€ These creative outputs underline the potential of LLMs in artistic and literary fields.

More complex reasoning abilities were revealed in mathematical and logical problem-solving tasks. For example, when provided with a train problem involving distances, speeds, and time calculations, the system not only produced the final answer but also explained the step-by-step process: calculating the distance covered by the first train, determining the relative speed, and finally computing the time and exact meeting point. This demonstrates how outputs can combine both reasoning and communication, yielding answers that are not only correct but also explainable.

In addition to text-based tasks, Generative AI also produced descriptive outputs for image generation. For instance, when given the prompt â€œA futuristic city with flying cars at sunsetâ€, the system generated a detailed description of a skyline filled with tall glass towers bathed in orange light, with flying cars moving along aerial lanes. While the description is textual here, image generation models can translate such descriptions into actual photorealistic or artistic visuals, further showcasing the multimodal output capabilities of generative systems.

Across all these cases, the outputs exhibited several consistent qualities. They were fluent and grammatically accurate, often indistinguishable from human writing. They displayed diversity, as repeating the same prompt with different sampling parameters produced varied but equally meaningful responses. They demonstrated high factual accuracy in structured domains such as mathematics and physics, while also showing creativity in open-ended tasks like poetry and storytelling. Most importantly, the outputs reflected the adaptability of generative modelsâ€”the same system could serve as a teacher, a programmer, a poet, or a problem solver depending on the prompt provided.

## Result
The experiment proved that Generative AI and Large Language Models can generate accurate, fluent, and creative outputs across diverse tasks. They simplified complex concepts into simple explanations, produced clean and executable code, solved logical problems step-by-step, and even generated poetic or descriptive content. The results highlight the adaptability and versatility of LLMs in handling knowledge, reasoning, and creativity simultaneously. This confirms that advancements in scaling and architecture directly enhance the performance and usefulness of these models.

